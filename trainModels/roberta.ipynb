{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device is available. Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "path = \"../trainedModels\"\n",
    "model_name = \"xlnet-base-cased\"\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS device is available. Using GPU for training.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA device is available. Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518927f10eb04c908704583e7907ae41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9027, 'grad_norm': 4.930896282196045, 'learning_rate': 4e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979ebd1850c046899b16154d8a12421f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8385895490646362, 'eval_accuracy': 0.6441784548422198, 'eval_precision': 0.6624169343363074, 'eval_recall': 0.6441784548422198, 'eval_f1': 0.5913067343294899, 'eval_runtime': 43.6275, 'eval_samples_per_second': 84.259, 'eval_steps_per_second': 2.636, 'epoch': 1.0}\n",
      "{'loss': 0.8202, 'grad_norm': 4.998886585235596, 'learning_rate': 3.371069182389938e-05, 'epoch': 1.09}\n",
      "{'loss': 0.7772, 'grad_norm': 4.992334365844727, 'learning_rate': 2.742138364779874e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a82dfe96f749ecb1241c8ec795dd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7798449397087097, 'eval_accuracy': 0.6583242655059848, 'eval_precision': 0.6776260514085894, 'eval_recall': 0.6583242655059848, 'eval_f1': 0.6057441428744403, 'eval_runtime': 43.7217, 'eval_samples_per_second': 84.077, 'eval_steps_per_second': 2.63, 'epoch': 2.0}\n",
      "{'loss': 0.7403, 'grad_norm': 6.736571788787842, 'learning_rate': 2.1132075471698115e-05, 'epoch': 2.17}\n",
      "{'loss': 0.6777, 'grad_norm': 5.069662094116211, 'learning_rate': 1.4842767295597484e-05, 'epoch': 2.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f50d49ba874936a8bcb6429f0b043f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7851482629776001, 'eval_accuracy': 0.6501632208922742, 'eval_precision': 0.6410801782519645, 'eval_recall': 0.6501632208922742, 'eval_f1': 0.6380137186119572, 'eval_runtime': 43.7024, 'eval_samples_per_second': 84.114, 'eval_steps_per_second': 2.631, 'epoch': 3.0}\n",
      "{'loss': 0.63, 'grad_norm': 7.8140058517456055, 'learning_rate': 8.553459119496857e-06, 'epoch': 3.26}\n",
      "{'loss': 0.5578, 'grad_norm': 6.438013076782227, 'learning_rate': 2.2641509433962266e-06, 'epoch': 3.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc2e4596a1b4dc89d21822c212dad5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8668508529663086, 'eval_accuracy': 0.6534276387377584, 'eval_precision': 0.6450559130263127, 'eval_recall': 0.6534276387377584, 'eval_f1': 0.6385866253073323, 'eval_runtime': 43.7739, 'eval_samples_per_second': 83.977, 'eval_steps_per_second': 2.627, 'epoch': 4.0}\n",
      "{'train_runtime': 4701.2477, 'train_samples_per_second': 25.025, 'train_steps_per_second': 0.783, 'train_loss': 0.7207895444787067, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5592696d68c4e82bb30eacfaf1389eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7629185318946838, 'eval_accuracy': 0.6682077780799565, 'eval_precision': 0.6803455789547144, 'eval_recall': 0.6682077780799565, 'eval_f1': 0.6204779445416811, 'eval_runtime': 43.8104, 'eval_samples_per_second': 83.93, 'eval_steps_per_second': 2.625, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files={'train': '../transformers_data/train.csv',\n",
    "                                         'validation': '../transformers_data/validation.csv',\n",
    "                                         'test': '../transformers_data/test.csv'})\n",
    "\n",
    "label_mapping = {\n",
    "    \"Effective\": 0,\n",
    "    \"Adequate\": 1,\n",
    "    \"Ineffective\": 2\n",
    "}\n",
    "\n",
    "label_mapping_types = {\n",
    "    \"Claim\": 0,\n",
    "    \"Concluding Statement\": 1,\n",
    "    \"Counterclaim\": 2,\n",
    "    \"Evidence\": 3,\n",
    "    \"Lead\": 4,\n",
    "    \"Position\": 5,\n",
    "    \"Rebuttal\": 6\n",
    "}\n",
    "\n",
    "def encode_labels(example):\n",
    "    example['labels'] = label_mapping[example['discourse_effectiveness']]\n",
    "    return example\n",
    "\n",
    "def encode_types(example):\n",
    "    example['types'] = label_mapping_types[example['discourse_type']]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(encode_labels)\n",
    "\n",
    "dataset = dataset.map(encode_types)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Ensure all entries are strings\n",
    "    texts = [text if isinstance(text, str) else \"\" for text in examples['discourse_text']]\n",
    "    \n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['discourse_id', 'essay_id', 'discourse_text', 'discourse_effectiveness', 'discourse_type'])  # Remove unnecessary columns\n",
    "\n",
    "tokenized_datasets.set_format('torch')\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=16)\n",
    "eval_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=16)\n",
    "\n",
    "num_labels = 3  # Effective, Adequate, Not Effective\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "model.to(device)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                      # Directory to save model checkpoints and outputs\n",
    "    eval_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    learning_rate=4e-5,                          # Learning rate for optimization\n",
    "    per_device_train_batch_size=32,              # Batch size per device during training\n",
    "    per_device_eval_batch_size=32,               # Batch size per device during evaluation\n",
    "    num_train_epochs=4,                          # Total number of training epochs\n",
    "    weight_decay=0.05,                           # Weight decay for regularization\n",
    "    load_best_model_at_end=True,                 # Load the best model when finished training\n",
    "    metric_for_best_model='accuracy',            # Metric to use for comparing models\n",
    "    save_strategy='epoch',                       # Save checkpoint every epoch\n",
    "    save_total_limit=2,                          # Limit the total amount of checkpoints\n",
    "    warmup_steps=500,                            # Set warmup steps for learning rate scheduling\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate(tokenized_datasets['test'])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e67bc0fb6984255a75da881b2ca96bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = \"xlnet-base-cased\"\n",
    "\n",
    "model.save_pretrained(f'{path}/{name}')\n",
    "\n",
    "tokenizer.save_pretrained(f'{path}/{name}')\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "# Save the Trainer object to a file\n",
    "with open(f'{path}/{name}/{name}.pkl', 'wb') as f:\n",
    "    pickle.dump(trainer.state.log_history, f)\n",
    "\n",
    "# Make predictions\n",
    "predictions = trainer.predict(tokenized_datasets['test'])\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "# Save the Trainer object to a file\n",
    "with open(f'{path}/{name}/{name}-predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
